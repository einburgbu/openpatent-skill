#!/usr/bin/env python3
"""
ä¸“åˆ©éƒ¨åˆ†ç”Ÿæˆå™¨ - é€šè¿‡ GLM API ç”Ÿæˆä¸“åˆ©ç« èŠ‚

ç”¨é€”ï¼šè§£å†³ Claude Code ç­‰ code å·¥å…·ä¸­çš„æ–‡æœ¬å‹ç¼©é—®é¢˜

å®‰è£…ä¾èµ–ï¼š
    pip install anthropic

é…ç½® API Keyï¼š
    export GLM_API_KEY="your-api-key-here"
    # æˆ–åˆ›å»º .env æ–‡ä»¶ï¼šGLM_API_KEY=your-key

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # ç”ŸæˆèƒŒæ™¯æŠ€æœ¯ï¼ˆä»…éœ€æŠ€æœ¯äº¤åº•ä¹¦ï¼‰
    python scripts/generate.py \\
        --prompt references/01_èƒŒæ™¯æŠ€æœ¯.md \\
        --context outputs/case_20250129/00_æŠ€æœ¯äº¤åº•ä¹¦.md \\
        --output outputs/case_20250129/01_èƒŒæ™¯æŠ€æœ¯.md

    # ç”Ÿæˆæƒåˆ©è¦æ±‚ä¹¦ï¼ˆéœ€è¦æŠ€æœ¯äº¤åº•ä¹¦ + èƒŒæ™¯æŠ€æœ¯ï¼‰
    python scripts/generate.py \\
        --prompt references/02_æƒè¦å¸ƒå±€.md \\
        --context outputs/case_20250129/00_æŠ€æœ¯äº¤åº•ä¹¦.md \\
        --context outputs/case_20250129/01_èƒŒæ™¯æŠ€æœ¯.md \\
        --output outputs/case_20250129/02_æƒåˆ©è¦æ±‚ä¹¦.md

    # æŒ‡å®šæ¨¡å‹ï¼ˆé»˜è®¤ glm-4.7ï¼‰
    python scripts/generate.py \\
        --model glm-4.7 \\
        --prompt references/01_èƒŒæ™¯æŠ€æœ¯.md \\
        --context outputs/case_20250129/00_æŠ€æœ¯äº¤åº•ä¹¦.md \\
        --output outputs/case_20250129/01_èƒŒæ™¯æŠ€æœ¯.md
"""

import os
import sys
import argparse
from pathlib import Path

try:
    from anthropic import Anthropic
except ImportError:
    print("é”™è¯¯: æœªå®‰è£… anthropic åº“")
    print("è¯·è¿è¡Œ: pip install anthropic")
    sys.exit(1)


# é»˜è®¤æ¨¡å‹é…ç½®
DEFAULT_MODEL = "glm-4.7"
MAX_TOKENS = 8192  # è¶³å¤Ÿç”Ÿæˆè¯¦ç»†çš„ä¸“åˆ©å†…å®¹

# GLM API ç«¯ç‚¹
GLM_BASE_URL = "https://open.bigmodel.cn/api/anthropic"


def get_api_key():
    """è·å– GLM API Key"""
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
    api_key = os.environ.get("GLM_API_KEY")
    if api_key:
        return api_key

    # å°è¯•ä» .env æ–‡ä»¶è¯»å–
    env_file = Path.cwd() / ".env"
    if env_file.exists():
        for line in env_file.read_text().strip().split("\n"):
            if line.startswith("GLM_API_KEY="):
                return line.split("=", 1)[1].strip()

    return None


def read_file_content(file_path: str) -> str:
    """è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå¤„ç†ç¼–ç é—®é¢˜"""
    path = Path(file_path)
    if not path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    # å°è¯•å¤šç§ç¼–ç 
    for encoding in ["utf-8", "gbk", "gb2312"]:
        try:
            return path.read_text(encoding=encoding)
        except UnicodeDecodeError:
            continue

    raise ValueError(f"æ— æ³•è¯»å–æ–‡ä»¶: {file_path}")


def build_user_message(prompt_template: str, context_files: list[str]) -> str:
    """
    æ„å»ºå‘é€ç»™ API çš„ç”¨æˆ·æ¶ˆæ¯

    æ ¼å¼ï¼šå°† prompt æ¨¡æ¿æ”¾åœ¨å‰é¢ï¼Œç„¶åé™„åŠ ä¸Šä¸‹æ–‡æ–‡ä»¶å†…å®¹
    """
    parts = []

    # é¦–å…ˆæ·»åŠ  prompt æ¨¡æ¿
    prompt_content = read_file_content(prompt_template)
    parts.append(prompt_content)

    # ç„¶åæ·»åŠ ä¸Šä¸‹æ–‡æ–‡ä»¶
    for ctx_file in context_files:
        ctx_content = read_file_content(ctx_file)
        filename = Path(ctx_file).name
        parts.append(f"\n\n## {filename}\n\n{ctx_content}")

    return "".join(parts)


def call_llm_api(prompt_template: str, context_files: list[str], model: str, temperature: float = 0.7) -> str:
    """
    è°ƒç”¨ GLM API ç”Ÿæˆå†…å®¹

    Args:
        prompt_template: prompt æ¨¡æ¿æ–‡ä»¶è·¯å¾„
        context_files: ä¸Šä¸‹æ–‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼‰ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§

    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
    """
    api_key = get_api_key()
    if not api_key:
        raise ValueError(
            "æœªæ‰¾åˆ° GLM_API_KEY\n"
            "è¯·è®¾ç½®ç¯å¢ƒå˜é‡æˆ–åˆ›å»º .env æ–‡ä»¶\n"
            "æ ¼å¼: GLM_API_KEY=your-key"
        )

    # ä½¿ç”¨è‡ªå®šä¹‰ base_url è¿æ¥ GLM API
    client = Anthropic(
        api_key=api_key,
        base_url=GLM_BASE_URL
    )

    # æ„å»ºæ¶ˆæ¯
    user_message = build_user_message(prompt_template, context_files)

    print(f"ğŸ“¤ æ­£åœ¨è°ƒç”¨æ¨¡å‹: {model}")
    print(f"ğŸŒ API ç«¯ç‚¹: {GLM_BASE_URL}")
    print(f"ğŸ“ è¾“å…¥ token æ•°çº¦: {len(user_message) // 3} (ä¼°ç®—)")

    # è°ƒç”¨ API
    response = client.messages.create(
        model=model,
        max_tokens=MAX_TOKENS,
        temperature=temperature,
        messages=[
            {
                "role": "user",
                "content": user_message
            }
        ]
    )

    content = response.content[0].text
    if hasattr(response.usage, 'output_tokens'):
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œè¾“å‡º token æ•°: {response.usage.output_tokens}")
    else:
        print(f"âœ… ç”Ÿæˆå®Œæˆ")

    return content


def post_process(content: str, output_path: str) -> None:
    """
    åå¤„ç†ç”Ÿæˆå†…å®¹

    1. ä¿å­˜ä¸»è¾“å‡ºæ–‡ä»¶
    2. å¤„ç†æƒåˆ©è¦æ±‚ä¹¦çš„è§£é‡Šéƒ¨åˆ†ï¼ˆå¦‚æœæœ‰ --- åˆ†éš”ç¬¦ï¼‰
    """
    output_file = Path(output_path)

    # ä¿å­˜ä¸»æ–‡ä»¶
    output_file.parent.mkdir(parents=True, exist_ok=True)
    output_file.write_text(content, encoding="utf-8")
    print(f"ğŸ’¾ å·²ä¿å­˜: {output_file}")

    # ç‰¹æ®Šå¤„ç†ï¼šæƒåˆ©è¦æ±‚ä¹¦çš„è§£é‡Šéƒ¨åˆ†
    if "æƒåˆ©è¦æ±‚ä¹¦" in output_file.name:
        if "---" in content:
            parts = content.split("---", 1)
            if len(parts) == 2:
                # ä¸»æ–‡ä»¶åªä¿ç•™ç¬¬ä¸€éƒ¨åˆ†
                main_content = parts[0].strip()
                output_file.write_text(main_content, encoding="utf-8")

                # è§£é‡Šéƒ¨åˆ†ä¿å­˜åˆ°å•ç‹¬æ–‡ä»¶
                explanation_path = output_file.parent / (output_file.stem + "_è§£é‡Š.md")
                explanation_content = parts[1].strip()
                explanation_path.write_text(explanation_content, encoding="utf-8")
                print(f"ğŸ’¾ è§£é‡Šéƒ¨åˆ†å·²åˆ†ç¦»: {explanation_path}")


def main():
    parser = argparse.ArgumentParser(
        description="ä¸“åˆ©éƒ¨åˆ†ç”Ÿæˆå™¨ - é€šè¿‡ GLM API ç”Ÿæˆä¸“åˆ©ç« èŠ‚",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ç”ŸæˆèƒŒæ™¯æŠ€æœ¯
  python scripts/generate.py \\
      --prompt references/01_èƒŒæ™¯æŠ€æœ¯.md \\
      --context outputs/case/00_æŠ€æœ¯äº¤åº•ä¹¦.md \\
      --output outputs/case/01_èƒŒæ™¯æŠ€æœ¯.md

  # æŒ‡å®šæ¨¡å‹
  python scripts/generate.py --model glm-4.7 --prompt ...

  # ç”Ÿæˆæƒåˆ©è¦æ±‚ä¹¦ï¼ˆå¤šä¸ªä¸Šä¸‹æ–‡ï¼‰
  python scripts/generate.py \\
      --prompt references/02_æƒè¦å¸ƒå±€.md \\
      --context outputs/case/00_æŠ€æœ¯äº¤åº•ä¹¦.md \\
      --context outputs/case/01_èƒŒæ™¯æŠ€æœ¯.md \\
      --output outputs/case/02_æƒåˆ©è¦æ±‚ä¹¦.md

  # è¾ƒä½æ¸©åº¦ï¼ˆæ›´ç¡®å®šæ€§çš„è¾“å‡ºï¼‰
  python scripts/generate.py --temperature 0.3 ...
        """
    )

    parser.add_argument(
        "--prompt", "-p",
        required=True,
        help="Prompt æ¨¡æ¿æ–‡ä»¶è·¯å¾„ (å¦‚ references/01_èƒŒæ™¯æŠ€æœ¯.md)"
    )
    parser.add_argument(
        "--context", "-c",
        action="append",
        default=[],
        help="ä¸Šä¸‹æ–‡æ–‡ä»¶è·¯å¾„ï¼ˆå¯å¤šæ¬¡ä½¿ç”¨ï¼ŒæŒ‰é¡ºåºæ·»åŠ ï¼‰"
    )
    parser.add_argument(
        "--output", "-o",
        required=True,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--model", "-m",
        default=DEFAULT_MODEL,
        help=f"æ¨¡å‹åç§°ï¼ˆé»˜è®¤: {DEFAULT_MODEL}ï¼‰"
    )
    parser.add_argument(
        "--temperature", "-t",
        type=float,
        default=0.7,
        help="æ¸©åº¦å‚æ•° 0-1ï¼Œé»˜è®¤ 0.7ã€‚è¾ƒä½å€¼è¾“å‡ºæ›´ç¡®å®šæ€§"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ä»…æ˜¾ç¤ºå°†è¦å‘é€çš„å†…å®¹ï¼Œä¸å®é™…è°ƒç”¨ API"
    )

    args = parser.parse_args()

    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not Path(args.prompt).exists():
        print(f"âŒ é”™è¯¯: Prompt æ–‡ä»¶ä¸å­˜åœ¨: {args.prompt}")
        sys.exit(1)

    for ctx_file in args.context:
        if not Path(ctx_file).exists():
            print(f"âŒ é”™è¯¯: ä¸Šä¸‹æ–‡æ–‡ä»¶ä¸å­˜åœ¨: {ctx_file}")
            sys.exit(1)

    # Dry run æ¨¡å¼
    if args.dry_run:
        print("=== Dry Run æ¨¡å¼ ===")
        print(f"æ¨¡å‹: {args.model}")
        print(f"API ç«¯ç‚¹: {GLM_BASE_URL}")
        print(f"æ¸©åº¦: {args.temperature}")
        print(f"Prompt æ¨¡æ¿: {args.prompt}")
        print(f"ä¸Šä¸‹æ–‡æ–‡ä»¶: {args.context}")
        print(f"è¾“å‡ºæ–‡ä»¶: {args.output}")
        print("\n=== å°†å‘é€çš„å†…å®¹ ===")
        print(build_user_message(args.prompt, args.context)[:1000] + "...")
        return

    try:
        # è°ƒç”¨ API ç”Ÿæˆå†…å®¹
        content = call_llm_api(
            prompt_template=args.prompt,
            context_files=args.context,
            model=args.model,
            temperature=args.temperature
        )

        # åå¤„ç†å’Œä¿å­˜
        post_process(content, args.output)

        print("\nâœ¨ ç”Ÿæˆå®Œæˆ!")

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
